{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cimple-chandu/cimple-chandu/blob/main/coffee.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "libraries\n"
      ],
      "metadata": {
        "id": "HfcUVtZpEIOG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cEnQMiVUChO1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from os import listdir\n",
        "from os.path import join, isfile\n",
        "import keras\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Dense, Dropout, Flatten,Activation\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "from keras.layers import Input, GlobalAveragePooling2D, concatenate, AveragePooling2D\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsvg0VNBMUse",
        "outputId": "b2970f03-dead-47bc-e792-5240521a8be7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"#Data Augmentation\"\"\"\n",
        "\n",
        "default_image_size = tuple((224, 224))\n",
        "directory_root = '/content/drive/MyDrive/coffee1'\n",
        "\n",
        "\n",
        "\n",
        "# horizontal flip\n",
        "def hflip(image_dir):\n",
        "  image = cv2.imread(image_dir)\n",
        "  image = cv2.flip(image, 0)\n",
        "  return convert_image_to_array(image)\n",
        "\n",
        "# vertical flip\n",
        "def vflip(image_dir):\n",
        "  image = cv2.imread(image_dir)\n",
        "  image = cv2.flip(image, 1)\n",
        "  return convert_image_to_array(image)\n",
        "\n",
        "# histogram equalization function\n",
        "def hist(img):\n",
        "  img_to_yuv = cv2.cvtColor(img,cv2.COLOR_BGR2YUV)\n",
        "  img_to_yuv[:,:,0] = cv2.equalizeHist(img_to_yuv[:,:,0])\n",
        "  hist_equalization_result = cv2.cvtColor(img_to_yuv, cv2.COLOR_YUV2BGR)\n",
        "  return hist_equalization_result\n",
        "\n",
        "#gaussian blurring\n",
        "from scipy import ndimage\n",
        "def gaussian(img):\n",
        "  img = ndimage.gaussian_filter(img, sigma= 5.11)\n",
        "  return img\n",
        "\n",
        "# function for rotation\n",
        "import random\n",
        "def rotation(img):\n",
        "  img = cv2.imread(img)\n",
        "  rows,cols = img.shape[0],img.shape[1]\n",
        "  randDeg = random.randint(-180, 180)\n",
        "  matrix = cv2.getRotationMatrix2D((cols/2, rows/2), randDeg, 0.70)\n",
        "  rotated = cv2.warpAffine(img, matrix, (rows, cols), borderMode=cv2.BORDER_CONSTANT)\n",
        "  return convert_image_to_array(rotated)\n",
        "\n",
        "def convert_image_to_array(image_dir):\n",
        "    try:\n",
        "        if type(image_dir) is str :\n",
        "            image = cv2.imread(image_dir)\n",
        "        else :\n",
        "            image = image_dir\n",
        "        if image is not None :\n",
        "            image = gaussian(image)\n",
        "            image = hist(image)\n",
        "            image=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "            image = cv2.resize(image, default_image_size)\n",
        "            return img_to_array(image)\n",
        "        else :\n",
        "            return np.array([])\n",
        "    except Exception as e:\n",
        "        print(f\"Error : {e}\")\n",
        "        return None\n",
        "\n",
        "\"\"\"#Load Dataset\"\"\"\n",
        "\n",
        "image_list, label_list = [], []\n",
        "\n",
        "try:\n",
        "    print(\"[INFO] Loading images ...\")\n",
        "    root_dir = listdir(directory_root)\n",
        "\n",
        "    for plant_folder in root_dir :\n",
        "        plant_disease_image_list = listdir(f\"{directory_root}/{plant_folder}\")\n",
        "\n",
        "        for image in plant_disease_image_list:\n",
        "            image_directory = f\"{directory_root}/{plant_folder}/{image}\"\n",
        "            image_list.append(convert_image_to_array(image_directory))\n",
        "            label_list.append(plant_folder)\n",
        "            image_list.append(hflip(image_directory))\n",
        "            label_list.append(plant_folder)\n",
        "            image_list.append(vflip(image_directory))\n",
        "            label_list.append(plant_folder)\n",
        "            image_list.append(rotation(image_directory))\n",
        "            label_list.append(plant_folder)\n",
        "    print(\"[INFO] Image loading completed\")\n",
        "except Exception as e:\n",
        "    print(f\"Error : {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_9jcukoEY-L",
        "outputId": "94885717-67f7-4e32-8237-8ecae1f4ede0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Loading images ...\n",
            "[INFO] Image loading completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_binarizer = LabelBinarizer()\n",
        "image_labels = label_binarizer.fit_transform(label_list)\n",
        "n_classes = len(label_binarizer.classes_)\n",
        "\n",
        "len(image_list)\n",
        "\n",
        "np_image_list = np.array(image_list, dtype=np.float16) / 225.0\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "print(\"[INFO] Spliting data to train, test\")\n",
        "x_train, x_test, y_train, y_test = train_test_split(np_image_list, image_labels, test_size=0.33, random_state = 42)\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "y_test = to_categorical(y_test)\n",
        "y_train = to_categorical(y_train)\n",
        "\n",
        "\n",
        "print(\"X_train shape : \",x_train.shape)\n",
        "print(\"y_train shape : \",y_train.shape)\n",
        "print(\"X_test shape : \",x_test.shape)\n",
        "print(\"y_test shape : \",y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJuYx3BJE8-B",
        "outputId": "1e5b47f9-3b01-4bf7-9590-919cb91f31af"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Spliting data to train, test\n",
            "X_train shape :  (726, 224, 224, 1)\n",
            "y_train shape :  (726, 2)\n",
            "X_test shape :  (358, 224, 224, 1)\n",
            "y_test shape :  (358, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocessing\n"
      ],
      "metadata": {
        "id": "TG2vR0DEObFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def conv_layer(conv_x, filters):\n",
        "    conv_x = BatchNormalization()(conv_x)\n",
        "    conv_x = Activation('relu')(conv_x)\n",
        "    conv_x = Conv2D(filters, (3, 3), kernel_initializer='he_uniform', padding='same', use_bias=False)(conv_x)\n",
        "    conv_x = Dropout(0.2)(conv_x)\n",
        "    return conv_x\n",
        "\n",
        "\n",
        "def dense_block(block_x, filters, growth_rate, layers_in_block):\n",
        "    for i in range(layers_in_block):\n",
        "        each_layer = conv_layer(block_x, growth_rate)\n",
        "        block_x = concatenate([block_x, each_layer], axis=-1)\n",
        "        filters += growth_rate\n",
        "\n",
        "    return block_x, filters\n",
        "\n",
        "\n",
        "def transition_block(trans_x, tran_filters):\n",
        "    trans_x = BatchNormalization()(trans_x)\n",
        "    trans_x = Activation('relu')(trans_x)\n",
        "    trans_x = Conv2D(tran_filters, (1, 1), kernel_initializer='he_uniform', padding='same', use_bias=False)(trans_x)\n",
        "    trans_x = AveragePooling2D((2, 2), strides=(2, 2))(trans_x)\n",
        "\n",
        "    return trans_x, tran_filters\n",
        "\n",
        "\n",
        "def dense_net(filters, growth_rate, classes, dense_block_size, layers_in_block):\n",
        "    input_img = Input(shape=(224, 224, 1))\n",
        "    x = Conv2D(24, (3, 3), kernel_initializer='he_uniform', padding='same', use_bias=False)(input_img)\n",
        "\n",
        "    dense_x = BatchNormalization()(x)\n",
        "    dense_x = Activation('relu')(x)\n",
        "\n",
        "    dense_x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='feature_layer')(dense_x)\n",
        "    for block in range(dense_block_size - 1):\n",
        "        dense_x, filters = dense_block(dense_x, filters, growth_rate, layers_in_block)\n",
        "        dense_x, filters = transition_block(dense_x, filters)\n",
        "\n",
        "    dense_x, filters = dense_block(dense_x, filters, growth_rate, layers_in_block)\n",
        "    dense_x = BatchNormalization()(dense_x)\n",
        "    dense_x = Activation('relu')(dense_x)\n",
        "    dense_x = GlobalAveragePooling2D()(dense_x)\n",
        "\n",
        "    output = Dense(classes, activation='softmax')(dense_x)\n",
        "\n",
        "    return Model(input_img, output)\n",
        "\n",
        "def densenet():\n",
        "  dense_block_size = 1\n",
        "  layers_in_block = 4\n",
        "\n",
        "  growth_rate = 12\n",
        "  classes = 2\n",
        "  model = dense_net(growth_rate * 2, growth_rate, classes, dense_block_size, layers_in_block)\n",
        "  return model\n",
        "\n",
        "\"\"\"**Convolution Neural Network**\"\"\"\n",
        "\n",
        "def cnn():\n",
        "  model = Sequential()\n",
        "  depth=1\n",
        "  height=224\n",
        "  width=224\n",
        "  inputShape = (height, width, depth)\n",
        "  chanDim = -1\n",
        "  if K.image_data_format() == \"channels_first\":\n",
        "      inputShape = (depth, height, width)\n",
        "      chanDim = 1\n",
        "  print(inputShape)\n",
        "  model.add(Conv2D(32, (3, 3), padding=\"same\",input_shape=inputShape))\n",
        "  model.add(Activation(\"relu\"))\n",
        "  model.add(BatchNormalization(axis=chanDim))\n",
        "  model.add(MaxPooling2D(pool_size=(3, 3)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "  model.add(Activation(\"relu\"))\n",
        "  model.add(BatchNormalization(axis=chanDim))\n",
        "  model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "  model.add(Activation(\"relu\"))\n",
        "  model.add(BatchNormalization(axis=chanDim))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
        "  model.add(Activation(\"relu\"))\n",
        "  model.add(BatchNormalization(axis=chanDim))\n",
        "  model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
        "  model.add(Activation(\"relu\"))\n",
        "  model.add(BatchNormalization(axis=chanDim))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2),name='feature_layer'))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1024))\n",
        "  model.add(Activation(\"relu\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(n_classes))\n",
        "  model.add(Activation(\"softmax\"))\n",
        "  return model"
      ],
      "metadata": {
        "id": "pt6PIMyOPCxM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"**ResNet**\"\"\"\n",
        "from keras.initializers import glorot_normal\n",
        "\n",
        "initializer = glorot_normal()\n",
        "\n",
        "\"\"\"\n",
        "Creates Residual Network with 50 layers\n",
        "\"\"\"\n",
        "from keras import layers\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\"\"\"\n",
        "Convolutional Block of ResNet\n",
        "\"\"\"\n",
        "def convolutional_block(X, f, filters, stage, block, s=2):\n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "\n",
        "    # First component of main path\n",
        "    X = layers.Conv2D(F1, (1, 1), strides=(s, s), name=conv_name_base + '2a',\n",
        "                            padding='same', kernel_initializer=initializer)(X)\n",
        "    X = layers.BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
        "    X = layers.Activation('relu')(X)\n",
        "    X = layers.Dropout(0.5)(X)\n",
        "\n",
        "    # Second component of main path\n",
        "    X = layers.Conv2D(F2, (f, f), strides=(1, 1), name=conv_name_base + '2b',\n",
        "                            padding='same', kernel_initializer=initializer)(X)\n",
        "    X = layers.BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
        "    X = layers.Activation('relu')(X)\n",
        "    X = layers.Dropout(0.5)(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = layers.Conv2D(F3, (1, 1), strides=(1, 1), name=conv_name_base + '2c',\n",
        "                            padding='same', kernel_initializer=initializer)(X)\n",
        "    X = layers.BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
        "\n",
        "    X_shortcut = layers.Conv2D(F3, (1, 1), strides=(s,s), name=conv_name_base + '1',\n",
        "                                    padding='same', kernel_initializer=initializer)(X_shortcut)\n",
        "    X_shortcut = layers.BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)\n",
        "\n",
        "    # Add shortcut value to main path, and pass it through a RELU activation\n",
        "    X = layers.Add()([X, X_shortcut])\n",
        "    X = layers.Activation('relu')(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def resnet(input_shape=(224, 224, 1), classes=2):\n",
        "    # Define the input as a tensor with shape input_shape\n",
        "\n",
        "    from keras import layers\n",
        "    from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Input\n",
        "    X_input = layers.Input(input_shape)\n",
        "\n",
        "\n",
        "\n",
        "    # Zero-Padding\n",
        "    X = layers.ZeroPadding2D((3, 3))(X_input)\n",
        "\n",
        "    # Stage 1\n",
        "    X = layers.Conv2D(64, (7, 7), strides=(2, 2), name='conv1',\n",
        "                            kernel_initializer=initializer)(X)\n",
        "    X = layers.BatchNormalization(axis=3, name='bn_conv1')(X)\n",
        "    X = layers.Activation('relu')(X)\n",
        "    X = layers.MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
        "\n",
        "    # Stage 2\n",
        "    X = convolutional_block(X, f = 3, filters=[64, 64, 256], stage=2, block='a', s=1)\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "    # Stage 3\n",
        "    X = convolutional_block(X, f = 3, filters=[128, 128, 512], stage=3, block='a', s=2)\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
        "\n",
        "    # Stage 4\n",
        "    X = convolutional_block(X, f = 3, filters=[256, 256, 1024], stage=4, block='a', s=2)\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n",
        "\n",
        "    # Stage 5\n",
        "    X = convolutional_block(X, f = 3, filters=[512, 512, 2048], stage=5, block='a', s=2)\n",
        "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n",
        "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n",
        "\n",
        "    # AVGPOOL\n",
        "    X = layers.AveragePooling2D(pool_size=(2, 2),name='feature_layer')(X)\n",
        "\n",
        "    # output layer\n",
        "    X = layers.Flatten()(X)\n",
        "    X = layers.Dense(classes, activation='softmax', name='fc{}'\n",
        "                            .format(classes), kernel_initializer=initializer)(X)\n",
        "\n",
        "    # Create model\n",
        "    model = keras.models.Model(inputs=X_input, outputs=X, name='resnet50')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "BWy1fYbZlXT5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identity_block(X, f, filters, stage, block):\n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    # Save the input value. You'll need this later to add back to the main path.\n",
        "    X_shortcut = X\n",
        "\n",
        "    # First component of main path\n",
        "    X = layers.Conv2D(filters=F1, kernel_size=(1, 1), strides=(1,1), padding='same',\n",
        "                            name=conv_name_base + '2a', kernel_initializer=initializer)(X)\n",
        "    X = layers.BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
        "    X = layers.Activation('relu')(X)\n",
        "    X = layers.Dropout(0.5)(X)\n",
        "\n",
        "    # Second component of main path\n",
        "    X = layers.Conv2D(filters=F2, kernel_size=(f, f), strides=(1,1), padding='same',\n",
        "                            name=conv_name_base + '2b', kernel_initializer=initializer)(X)\n",
        "    X = layers.BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
        "    X = layers.Activation('relu')(X)\n",
        "    X = layers.Dropout(0.5)(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = layers.Conv2D(filters=F3, kernel_size=(1, 1), strides=(1,1), padding='same',\n",
        "                            name=conv_name_base + '2c', kernel_initializer=initializer)(X)\n",
        "    X = layers.BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
        "\n",
        "    # Add shortcut value to main path, and pass it through a RELU activation\n",
        "    X = layers.Add()([X, X_shortcut])\n",
        "    X = layers.Activation('relu')(X)\n",
        "\n",
        "    return X"
      ],
      "metadata": {
        "id": "bXU6FOjir8pa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sum(model):\n",
        "  model.summary()\n",
        "\n",
        "def comp(model):\n",
        "    from keras.optimizers import Adam\n",
        "    # Initialize Adam with learning_rate\n",
        "    opt = Adam(learning_rate=0.001)\n",
        "    # Compile the model using the initialized optimizer\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\"\"\"def fitmodel(model):\n",
        "  hist=model.fit(x_train,y_train, epochs=16, batch_size=32, shuffle=True,\n",
        "                    validation_data=(x_test, y_test), validation_steps=16)\n",
        "  return hist\"\"\"\n",
        "\n",
        "def fitmodel(model):\n",
        "    history = model.fit(x_train, y_train, epochs=20, validation_data=(x_test, y_test), batch_size=32)\n",
        "    return history\n",
        "\n",
        "\n",
        "\"\"\"#Define Performance Metrics for DL models\"\"\"\n",
        "\n",
        "def performance(model):\n",
        "  print(\"[INFO] Calculating model accuracy\")\n",
        "  scores = model.evaluate(x_test, y_test)\n",
        "  print(f\"Test Accuracy: {scores[1]*100}\")\n",
        "  scores = model.evaluate(x_test, y_test)\n",
        "  print(f\"Train Accuracy: {model.evaluate(x_train,y_train)[1]*100}\")\n",
        "\n",
        "def graph(hist):\n",
        "  acc = hist.history['accuracy']\n",
        "  val_acc = hist.history['val_accuracy']\n",
        "  loss = hist.history['loss']\n",
        "  val_loss = hist.history['val_loss']\n",
        "  epochs = range(1, len(acc) + 1)\n",
        "  #Train and validation accuracy\n",
        "  plt.plot(epochs, acc, 'b', label='Training accurarcy')\n",
        "  plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')\n",
        "  plt.title('Training and Validation accurarcy')\n",
        "  plt.legend()\n",
        "  plt.figure()\n",
        "  #Train and validation loss\n",
        "  plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "  plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "  plt.title('Training and Validation loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "evxVl4hTx0z8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"**CNN**\"\"\"\n",
        "\n",
        "model_cnn=cnn()\n",
        "sum(model_cnn)\n",
        "comp(model_cnn)\n",
        "hist_cnn=fitmodel(model_cnn)\n",
        "performance(model_cnn)\n",
        "#graph(hist_cnn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fN_LKShShq9n",
        "outputId": "3da32775-3854-4a97-ed89-aabf1db5fa73"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(224, 224, 1)\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_15 (Conv2D)          (None, 224, 224, 32)      320       \n",
            "                                                                 \n",
            " activation_20 (Activation)  (None, 224, 224, 32)      0         \n",
            "                                                                 \n",
            " batch_normalization_18 (Ba  (None, 224, 224, 32)      128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPoolin  (None, 74, 74, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 74, 74, 32)        0         \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, 74, 74, 64)        18496     \n",
            "                                                                 \n",
            " activation_21 (Activation)  (None, 74, 74, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_19 (Ba  (None, 74, 74, 64)        256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 74, 74, 64)        36928     \n",
            "                                                                 \n",
            " activation_22 (Activation)  (None, 74, 74, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_20 (Ba  (None, 74, 74, 64)        256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPoolin  (None, 37, 37, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 37, 37, 64)        0         \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          (None, 37, 37, 128)       73856     \n",
            "                                                                 \n",
            " activation_23 (Activation)  (None, 37, 37, 128)       0         \n",
            "                                                                 \n",
            " batch_normalization_21 (Ba  (None, 37, 37, 128)       512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 37, 37, 128)       147584    \n",
            "                                                                 \n",
            " activation_24 (Activation)  (None, 37, 37, 128)       0         \n",
            "                                                                 \n",
            " batch_normalization_22 (Ba  (None, 37, 37, 128)       512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " feature_layer (MaxPooling2  (None, 18, 18, 128)       0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 18, 18, 128)       0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 41472)             0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1024)              42468352  \n",
            "                                                                 \n",
            " activation_25 (Activation)  (None, 1024)              0         \n",
            "                                                                 \n",
            " batch_normalization_23 (Ba  (None, 1024)              4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 2)                 2050      \n",
            "                                                                 \n",
            " activation_26 (Activation)  (None, 2)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 42753346 (163.09 MB)\n",
            "Trainable params: 42750466 (163.08 MB)\n",
            "Non-trainable params: 2880 (11.25 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/16\n",
            "23/23 [==============================] - ETA: 0s - loss: 1.3788 - accuracy: 0.5496"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 16 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r23/23 [==============================] - 20s 776ms/step - loss: 1.3788 - accuracy: 0.5496 - val_loss: 3.1403 - val_accuracy: 0.7933\n",
            "Epoch 2/16\n",
            "23/23 [==============================] - 16s 687ms/step - loss: 0.7104 - accuracy: 0.6983\n",
            "Epoch 3/16\n",
            "23/23 [==============================] - 16s 688ms/step - loss: 0.5935 - accuracy: 0.7479\n",
            "Epoch 4/16\n",
            "23/23 [==============================] - 16s 686ms/step - loss: 0.5008 - accuracy: 0.7920\n",
            "Epoch 5/16\n",
            "23/23 [==============================] - 16s 686ms/step - loss: 0.4407 - accuracy: 0.8030\n",
            "Epoch 6/16\n",
            "23/23 [==============================] - 16s 691ms/step - loss: 0.4495 - accuracy: 0.8223\n",
            "Epoch 7/16\n",
            "23/23 [==============================] - 16s 680ms/step - loss: 0.3048 - accuracy: 0.8815\n",
            "Epoch 8/16\n",
            "23/23 [==============================] - 16s 697ms/step - loss: 0.2852 - accuracy: 0.8939\n",
            "Epoch 9/16\n",
            "23/23 [==============================] - 16s 687ms/step - loss: 0.2859 - accuracy: 0.8760\n",
            "Epoch 10/16\n",
            "23/23 [==============================] - 16s 687ms/step - loss: 0.3093 - accuracy: 0.8981\n",
            "Epoch 11/16\n",
            "23/23 [==============================] - 16s 678ms/step - loss: 0.2404 - accuracy: 0.9022\n",
            "Epoch 12/16\n",
            "23/23 [==============================] - 16s 690ms/step - loss: 0.2001 - accuracy: 0.9160\n",
            "Epoch 13/16\n",
            "23/23 [==============================] - 16s 700ms/step - loss: 0.1587 - accuracy: 0.9421\n",
            "Epoch 14/16\n",
            "23/23 [==============================] - 16s 693ms/step - loss: 0.1479 - accuracy: 0.9545\n",
            "Epoch 15/16\n",
            "23/23 [==============================] - 16s 693ms/step - loss: 0.1169 - accuracy: 0.9642\n",
            "Epoch 16/16\n",
            "23/23 [==============================] - 16s 699ms/step - loss: 0.1213 - accuracy: 0.9518\n",
            "[INFO] Calculating model accuracy\n",
            "12/12 [==============================] - 1s 122ms/step - loss: 16.2256 - accuracy: 0.7933\n",
            "Test Accuracy: 79.32960987091064\n",
            "12/12 [==============================] - 1s 122ms/step - loss: 16.2256 - accuracy: 0.7933\n",
            "23/23 [==============================] - 3s 123ms/step - loss: 17.7354 - accuracy: 0.7603\n",
            "Train Accuracy: 76.03305578231812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Assuming `model` is your trained model\n",
        "joblib.dump(model_cnn, '/content/drive/My Drive/model_cnn.pkl')  # This will overwrite any existing file at that path.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAcisajLhi6g",
        "outputId": "d34cc6a6-0ef4-4373-b742-a3a43321feb7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/model_cnn.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"dense net\"\"\"\n",
        "model_densenet=densenet()\n",
        "sum(model_densenet)\n",
        "comp(model_densenet)\n",
        "hist_densenet=fitmodel(model_densenet)\n",
        "performance(model_densenet)\n",
        "graph(hist_densenet)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IH7qo5AthN4b",
        "outputId": "4194c6f8-bd44-4ce0-858a-fd37480dbed8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 1)]        0         []                            \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)          (None, 224, 224, 24)         216       ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " activation_27 (Activation)  (None, 224, 224, 24)         0         ['conv2d_20[0][0]']           \n",
            "                                                                                                  \n",
            " feature_layer (MaxPooling2  (None, 112, 112, 24)         0         ['activation_27[0][0]']       \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_25 (Ba  (None, 112, 112, 24)         96        ['feature_layer[0][0]']       \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_28 (Activation)  (None, 112, 112, 24)         0         ['batch_normalization_25[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)          (None, 112, 112, 12)         2592      ['activation_28[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)        (None, 112, 112, 12)         0         ['conv2d_21[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate  (None, 112, 112, 36)         0         ['feature_layer[0][0]',       \n",
            " )                                                                   'dropout_16[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_26 (Ba  (None, 112, 112, 36)         144       ['concatenate_4[0][0]']       \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_29 (Activation)  (None, 112, 112, 36)         0         ['batch_normalization_26[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)          (None, 112, 112, 12)         3888      ['activation_29[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)        (None, 112, 112, 12)         0         ['conv2d_22[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate  (None, 112, 112, 48)         0         ['concatenate_4[0][0]',       \n",
            " )                                                                   'dropout_17[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_27 (Ba  (None, 112, 112, 48)         192       ['concatenate_5[0][0]']       \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_30 (Activation)  (None, 112, 112, 48)         0         ['batch_normalization_27[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)          (None, 112, 112, 12)         5184      ['activation_30[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_18 (Dropout)        (None, 112, 112, 12)         0         ['conv2d_23[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate  (None, 112, 112, 60)         0         ['concatenate_5[0][0]',       \n",
            " )                                                                   'dropout_18[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_28 (Ba  (None, 112, 112, 60)         240       ['concatenate_6[0][0]']       \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_31 (Activation)  (None, 112, 112, 60)         0         ['batch_normalization_28[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)          (None, 112, 112, 12)         6480      ['activation_31[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)        (None, 112, 112, 12)         0         ['conv2d_24[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_7 (Concatenate  (None, 112, 112, 72)         0         ['concatenate_6[0][0]',       \n",
            " )                                                                   'dropout_19[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_29 (Ba  (None, 112, 112, 72)         288       ['concatenate_7[0][0]']       \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_32 (Activation)  (None, 112, 112, 72)         0         ['batch_normalization_29[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " global_average_pooling2d_1  (None, 72)                   0         ['activation_32[0][0]']       \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 2)                    146       ['global_average_pooling2d_1[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 19466 (76.04 KB)\n",
            "Trainable params: 18986 (74.16 KB)\n",
            "Non-trainable params: 480 (1.88 KB)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/16\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.5580 - accuracy: 0.7603"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 16 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r23/23 [==============================] - 29s 1s/step - loss: 0.5580 - accuracy: 0.7603 - val_loss: 0.5157 - val_accuracy: 0.7933\n",
            "Epoch 2/16\n",
            "23/23 [==============================] - 24s 1s/step - loss: 0.5359 - accuracy: 0.7603\n",
            "Epoch 3/16\n",
            "23/23 [==============================] - 25s 1s/step - loss: 0.5262 - accuracy: 0.7603\n",
            "Epoch 4/16\n",
            "23/23 [==============================] - 25s 1s/step - loss: 0.5209 - accuracy: 0.7603\n",
            "Epoch 5/16\n",
            "23/23 [==============================] - 24s 1s/step - loss: 0.5171 - accuracy: 0.7603\n",
            "Epoch 6/16\n",
            "23/23 [==============================] - 25s 1s/step - loss: 0.5117 - accuracy: 0.7603\n",
            "Epoch 7/16\n",
            "23/23 [==============================] - 24s 1s/step - loss: 0.5083 - accuracy: 0.7603\n",
            "Epoch 8/16\n",
            "23/23 [==============================] - 24s 1s/step - loss: 0.5047 - accuracy: 0.7603\n",
            "Epoch 9/16\n",
            "23/23 [==============================] - 25s 1s/step - loss: 0.5079 - accuracy: 0.7713\n",
            "Epoch 10/16\n",
            "23/23 [==============================] - 24s 1s/step - loss: 0.5076 - accuracy: 0.7617\n",
            "Epoch 11/16\n",
            "23/23 [==============================] - 25s 1s/step - loss: 0.4992 - accuracy: 0.7603\n",
            "Epoch 12/16\n",
            "23/23 [==============================] - 24s 1s/step - loss: 0.4863 - accuracy: 0.7713\n",
            "Epoch 13/16\n",
            "23/23 [==============================] - 24s 1s/step - loss: 0.4823 - accuracy: 0.7727\n",
            "Epoch 14/16\n",
            "23/23 [==============================] - 24s 1s/step - loss: 0.4902 - accuracy: 0.7755\n",
            "Epoch 15/16\n",
            "23/23 [==============================] - 25s 1s/step - loss: 0.4804 - accuracy: 0.7700\n",
            "Epoch 16/16\n",
            "23/23 [==============================] - 25s 1s/step - loss: 0.4884 - accuracy: 0.7727\n",
            "[INFO] Calculating model accuracy\n",
            "12/12 [==============================] - 2s 179ms/step - loss: 0.7425 - accuracy: 0.3492\n",
            "Test Accuracy: 34.91620123386383\n",
            "12/12 [==============================] - 2s 177ms/step - loss: 0.7425 - accuracy: 0.3492\n",
            "23/23 [==============================] - 4s 182ms/step - loss: 0.7193 - accuracy: 0.3994\n",
            "Train Accuracy: 39.944905042648315\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "x and y must have same first dimension, but have shapes (16,) and (1,)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ada737b57e3d>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhist_densenet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfitmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_densenet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mperformance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_densenet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_densenet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-b9b79486c2d1>\u001b[0m in \u001b[0;36mgraph\u001b[0;34m(hist)\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;31m#Train and validation accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training accurarcy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation accurarcy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training and Validation accurarcy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3577\u001b[0m ) -> list[Line2D]:\n\u001b[0;32m-> 3578\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   3579\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3580\u001b[0m         \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1719\u001b[0m         \"\"\"\n\u001b[1;32m   1720\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1721\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1722\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    304\u001b[0m                 axes, this, kwargs, ambiguous_fmt_datakey=ambiguous_fmt_datakey)\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    500\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (16,) and (1,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Assuming `model` is your trained model\n",
        "joblib.dump(model_densenet, '/content/drive/My Drive/model_densenet.pkl')  # This will overwrite any existing file at that path.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqjaBfsqnAEY",
        "outputId": "68334827-25be-45b7-fcfc-570a4499a3f7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/model_densenet.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"**ResNet**\"\"\"\n",
        "\n",
        "model_resnet=resnet()\n",
        "sum(model_resnet)\n",
        "comp(model_resnet)\n",
        "hist_resnet=fitmodel(model_resnet)\n",
        "performance(model_resnet)\n",
        "#graph(hist_resnet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyIdRY3jrA9x",
        "outputId": "6423b5d6-33ae-42f7-abf0-89fe168fb7c1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None, 224, 224, 1)]        0         []                            \n",
            "                                                                                                  \n",
            " zero_padding2d (ZeroPaddin  (None, 230, 230, 1)          0         ['input_3[0][0]']             \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv1 (Conv2D)              (None, 112, 112, 64)         3200      ['zero_padding2d[0][0]']      \n",
            "                                                                                                  \n",
            " bn_conv1 (BatchNormalizati  (None, 112, 112, 64)         256       ['conv1[0][0]']               \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " activation_33 (Activation)  (None, 112, 112, 64)         0         ['bn_conv1[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling2d_6 (MaxPoolin  (None, 55, 55, 64)           0         ['activation_33[0][0]']       \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " res2a_branch2a (Conv2D)     (None, 55, 55, 64)           4160      ['max_pooling2d_6[0][0]']     \n",
            "                                                                                                  \n",
            " bn2a_branch2a (BatchNormal  (None, 55, 55, 64)           256       ['res2a_branch2a[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_34 (Activation)  (None, 55, 55, 64)           0         ['bn2a_branch2a[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_20 (Dropout)        (None, 55, 55, 64)           0         ['activation_34[0][0]']       \n",
            "                                                                                                  \n",
            " res2a_branch2b (Conv2D)     (None, 55, 55, 64)           36928     ['dropout_20[0][0]']          \n",
            "                                                                                                  \n",
            " bn2a_branch2b (BatchNormal  (None, 55, 55, 64)           256       ['res2a_branch2b[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_35 (Activation)  (None, 55, 55, 64)           0         ['bn2a_branch2b[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_21 (Dropout)        (None, 55, 55, 64)           0         ['activation_35[0][0]']       \n",
            "                                                                                                  \n",
            " res2a_branch2c (Conv2D)     (None, 55, 55, 256)          16640     ['dropout_21[0][0]']          \n",
            "                                                                                                  \n",
            " res2a_branch1 (Conv2D)      (None, 55, 55, 256)          16640     ['max_pooling2d_6[0][0]']     \n",
            "                                                                                                  \n",
            " bn2a_branch2c (BatchNormal  (None, 55, 55, 256)          1024      ['res2a_branch2c[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " bn2a_branch1 (BatchNormali  (None, 55, 55, 256)          1024      ['res2a_branch1[0][0]']       \n",
            " zation)                                                                                          \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 55, 55, 256)          0         ['bn2a_branch2c[0][0]',       \n",
            "                                                                     'bn2a_branch1[0][0]']        \n",
            "                                                                                                  \n",
            " activation_36 (Activation)  (None, 55, 55, 256)          0         ['add[0][0]']                 \n",
            "                                                                                                  \n",
            " res2b_branch2a (Conv2D)     (None, 55, 55, 64)           16448     ['activation_36[0][0]']       \n",
            "                                                                                                  \n",
            " bn2b_branch2a (BatchNormal  (None, 55, 55, 64)           256       ['res2b_branch2a[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_37 (Activation)  (None, 55, 55, 64)           0         ['bn2b_branch2a[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_22 (Dropout)        (None, 55, 55, 64)           0         ['activation_37[0][0]']       \n",
            "                                                                                                  \n",
            " res2b_branch2b (Conv2D)     (None, 55, 55, 64)           36928     ['dropout_22[0][0]']          \n",
            "                                                                                                  \n",
            " bn2b_branch2b (BatchNormal  (None, 55, 55, 64)           256       ['res2b_branch2b[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_38 (Activation)  (None, 55, 55, 64)           0         ['bn2b_branch2b[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_23 (Dropout)        (None, 55, 55, 64)           0         ['activation_38[0][0]']       \n",
            "                                                                                                  \n",
            " res2b_branch2c (Conv2D)     (None, 55, 55, 256)          16640     ['dropout_23[0][0]']          \n",
            "                                                                                                  \n",
            " bn2b_branch2c (BatchNormal  (None, 55, 55, 256)          1024      ['res2b_branch2c[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 55, 55, 256)          0         ['bn2b_branch2c[0][0]',       \n",
            "                                                                     'activation_36[0][0]']       \n",
            "                                                                                                  \n",
            " activation_39 (Activation)  (None, 55, 55, 256)          0         ['add_1[0][0]']               \n",
            "                                                                                                  \n",
            " res2c_branch2a (Conv2D)     (None, 55, 55, 64)           16448     ['activation_39[0][0]']       \n",
            "                                                                                                  \n",
            " bn2c_branch2a (BatchNormal  (None, 55, 55, 64)           256       ['res2c_branch2a[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_40 (Activation)  (None, 55, 55, 64)           0         ['bn2c_branch2a[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_24 (Dropout)        (None, 55, 55, 64)           0         ['activation_40[0][0]']       \n",
            "                                                                                                  \n",
            " res2c_branch2b (Conv2D)     (None, 55, 55, 64)           36928     ['dropout_24[0][0]']          \n",
            "                                                                                                  \n",
            " bn2c_branch2b (BatchNormal  (None, 55, 55, 64)           256       ['res2c_branch2b[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_41 (Activation)  (None, 55, 55, 64)           0         ['bn2c_branch2b[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_25 (Dropout)        (None, 55, 55, 64)           0         ['activation_41[0][0]']       \n",
            "                                                                                                  \n",
            " res2c_branch2c (Conv2D)     (None, 55, 55, 256)          16640     ['dropout_25[0][0]']          \n",
            "                                                                                                  \n",
            " bn2c_branch2c (BatchNormal  (None, 55, 55, 256)          1024      ['res2c_branch2c[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 55, 55, 256)          0         ['bn2c_branch2c[0][0]',       \n",
            "                                                                     'activation_39[0][0]']       \n",
            "                                                                                                  \n",
            " activation_42 (Activation)  (None, 55, 55, 256)          0         ['add_2[0][0]']               \n",
            "                                                                                                  \n",
            " res3a_branch2a (Conv2D)     (None, 28, 28, 128)          32896     ['activation_42[0][0]']       \n",
            "                                                                                                  \n",
            " bn3a_branch2a (BatchNormal  (None, 28, 28, 128)          512       ['res3a_branch2a[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_43 (Activation)  (None, 28, 28, 128)          0         ['bn3a_branch2a[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_26 (Dropout)        (None, 28, 28, 128)          0         ['activation_43[0][0]']       \n",
            "                                                                                                  \n",
            " res3a_branch2b (Conv2D)     (None, 28, 28, 128)          147584    ['dropout_26[0][0]']          \n",
            "                                                                                                  \n",
            " bn3a_branch2b (BatchNormal  (None, 28, 28, 128)          512       ['res3a_branch2b[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_44 (Activation)  (None, 28, 28, 128)          0         ['bn3a_branch2b[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_27 (Dropout)        (None, 28, 28, 128)          0         ['activation_44[0][0]']       \n",
            "                                                                                                  \n",
            " res3a_branch2c (Conv2D)     (None, 28, 28, 512)          66048     ['dropout_27[0][0]']          \n",
            "                                                                                                  \n",
            " res3a_branch1 (Conv2D)      (None, 28, 28, 512)          131584    ['activation_42[0][0]']       \n",
            "                                                                                                  \n",
            " bn3a_branch2c (BatchNormal  (None, 28, 28, 512)          2048      ['res3a_branch2c[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " bn3a_branch1 (BatchNormali  (None, 28, 28, 512)          2048      ['res3a_branch1[0][0]']       \n",
            " zation)                                                                                          \n",
            "                                                                                                  \n",
            " add_3 (Add)                 (None, 28, 28, 512)          0         ['bn3a_branch2c[0][0]',       \n",
            "                                                                     'bn3a_branch1[0][0]']        \n",
            "                                                                                                  \n",
            " activation_45 (Activation)  (None, 28, 28, 512)          0         ['add_3[0][0]']               \n",
            "                                                                                                  \n",
            " res3b_branch2a (Conv2D)     (None, 28, 28, 128)          65664     ['activation_45[0][0]']       \n",
            "                                                                                                  \n",
            " bn3b_branch2a (BatchNormal  (None, 28, 28, 128)          512       ['res3b_branch2a[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_46 (Activation)  (None, 28, 28, 128)          0         ['bn3b_branch2a[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_28 (Dropout)        (None, 28, 28, 128)          0         ['activation_46[0][0]']       \n",
            "                                                                                                  \n",
            " res3b_branch2b (Conv2D)     (None, 28, 28, 128)          147584    ['dropout_28[0][0]']          \n",
            "                                                                                                  \n",
            " bn3b_branch2b (BatchNormal  (None, 28, 28, 128)          512       ['res3b_branch2b[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_47 (Activation)  (None, 28, 28, 128)          0         ['bn3b_branch2b[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_29 (Dropout)        (None, 28, 28, 128)          0         ['activation_47[0][0]']       \n",
            "                                                                                                  \n",
            " res3b_branch2c (Conv2D)     (None, 28, 28, 512)          66048     ['dropout_29[0][0]']          \n",
            "                                                                                                  \n",
            " bn3b_branch2c (BatchNormal  (None, 28, 28, 512)          2048      ['res3b_branch2c[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_4 (Add)                 (None, 28, 28, 512)          0         ['bn3b_branch2c[0][0]',       \n",
            "                                                                     'activation_45[0][0]']       \n",
            "                                                                                                  \n",
            " activation_48 (Activation)  (None, 28, 28, 512)          0         ['add_4[0][0]']               \n",
            "                                                                                                  \n",
            " res3c_branch2a (Conv2D)     (None, 28, 28, 128)          65664     ['activation_48[0][0]']       \n",
            "                                                                                                  \n",
            " bn3c_branch2a (BatchNormal  (None, 28, 28, 128)          512       ['res3c_branch2a[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_49 (Activation)  (None, 28, 28, 128)          0         ['bn3c_branch2a[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_30 (Dropout)        (None, 28, 28, 128)          0         ['activation_49[0][0]']       \n",
            "                                                                                                  \n",
            " res3c_branch2b (Conv2D)     (None, 28, 28, 128)          147584    ['dropout_30[0][0]']          \n",
            "                                                                                                  \n",
            " bn3c_branch2b (BatchNormal  (None, 28, 28, 128)          512       ['res3c_branch2b[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_50 (Activation)  (None, 28, 28, 128)          0         ['bn3c_branch2b[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_31 (Dropout)        (None, 28, 28, 128)          0         ['activation_50[0][0]']       \n",
            "                                                                                                  \n",
            " res3c_branch2c (Conv2D)     (None, 28, 28, 512)          66048     ['dropout_31[0][0]']          \n",
            "                                                                                                  \n",
            " bn3c_branch2c (BatchNormal  (None, 28, 28, 512)          2048      ['res3c_branch2c[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_5 (Add)                 (None, 28, 28, 512)          0         ['bn3c_branch2c[0][0]',       \n",
            "                                                                     'activation_48[0][0]']       \n",
            "                                                                                                  \n",
            " activation_51 (Activation)  (None, 28, 28, 512)          0         ['add_5[0][0]']               \n",
            "                                                                                                  \n",
            " res3d_branch2a (Conv2D)     (None, 28, 28, 128)          65664     ['activation_51[0][0]']       \n",
            "                                                                                                  \n",
            " bn3d_branch2a (BatchNormal  (None, 28, 28, 128)          512       ['res3d_branch2a[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_52 (Activation)  (None, 28, 28, 128)          0         ['bn3d_branch2a[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_32 (Dropout)        (None, 28, 28, 128)          0         ['activation_52[0][0]']       \n",
            "                                                                                                  \n",
            " res3d_branch2b (Conv2D)     (None, 28, 28, 128)          147584    ['dropout_32[0][0]']          \n",
            "                                                                                                  \n",
            " bn3d_branch2b (BatchNormal  (None, 28, 28, 128)          512       ['res3d_branch2b[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_53 (Activation)  (None, 28, 28, 128)          0         ['bn3d_branch2b[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_33 (Dropout)        (None, 28, 28, 128)          0         ['activation_53[0][0]']       \n",
            "                                                                                                  \n",
            " res3d_branch2c (Conv2D)     (None, 28, 28, 512)          66048     ['dropout_33[0][0]']          \n",
            "                                                                                                  \n",
            " bn3d_branch2c (BatchNormal  (None, 28, 28, 512)          2048      ['res3d_branch2c[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_6 (Add)                 (None, 28, 28, 512)          0         ['bn3d_branch2c[0][0]',       \n",
            "                                                                     'activation_51[0][0]']       \n",
            "                                                                                                  \n",
            " activation_54 (Activation)  (None, 28, 28, 512)          0         ['add_6[0][0]']               \n",
            "                                                                                                  \n",
            " res4a_branch2a (Conv2D)     (None, 14, 14, 256)          131328    ['activation_54[0][0]']       \n",
            "                                                                                                  \n",
            " bn4a_branch2a (BatchNormal  (None, 14, 14, 256)          1024      ['res4a_branch2a[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_55 (Activation)  (None, 14, 14, 256)          0         ['bn4a_branch2a[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_34 (Dropout)        (None, 14, 14, 256)          0         ['activation_55[0][0]']       \n",
            "                                                                                                  \n",
            " res4a_branch2b (Conv2D)     (None, 14, 14, 256)          590080    ['dropout_34[0][0]']          \n",
            "                                                                                                  \n",
            " bn4a_branch2b (BatchNormal  (None, 14, 14, 256)          1024      ['res4a_branch2b[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_56 (Activation)  (None, 14, 14, 256)          0         ['bn4a_branch2b[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_35 (Dropout)        (None, 14, 14, 256)          0         ['activation_56[0][0]']       \n",
            "                                                                                                  \n",
            " res4a_branch2c (Conv2D)     (None, 14, 14, 1024)         263168    ['dropout_35[0][0]']          \n",
            "                                                                                                  \n",
            " res4a_branch1 (Conv2D)      (None, 14, 14, 1024)         525312    ['activation_54[0][0]']       \n",
            "                                                                                                  \n",
            " bn4a_branch2c (BatchNormal  (None, 14, 14, 1024)         4096      ['res4a_branch2c[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " bn4a_branch1 (BatchNormali  (None, 14, 14, 1024)         4096      ['res4a_branch1[0][0]']       \n",
            " zation)                                                                                          \n",
            "                                                                                                  \n",
            " add_7 (Add)                 (None, 14, 14, 1024)         0         ['bn4a_branch2c[0][0]',       \n",
            "                                                                     'bn4a_branch1[0][0]']        \n",
            "                                                                                                  \n",
            " activation_57 (Activation)  (None, 14, 14, 1024)         0         ['add_7[0][0]']               \n",
            "                                                                                                  \n",
            " res4b_branch2a (Conv2D)     (None, 14, 14, 256)          262400    ['activation_57[0][0]']       \n",
            "                                                                                                  \n",
            " bn4b_branch2a (BatchNormal  (None, 14, 14, 256)          1024      ['res4b_branch2a[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_58 (Activation)  (None, 14, 14, 256)          0         ['bn4b_branch2a[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_36 (Dropout)        (None, 14, 14, 256)          0         ['activation_58[0][0]']       \n",
            "                                                                                                  \n",
            " res4b_branch2b (Conv2D)     (None, 14, 14, 256)          590080    ['dropout_36[0][0]']          \n",
            "                                                                                                  \n",
            " bn4b_branch2b (BatchNormal  (None, 14, 14, 256)          1024      ['res4b_branch2b[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_59 (Activation)  (None, 14, 14, 256)          0         ['bn4b_branch2b[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)        (None, 14, 14, 256)          0         ['activation_59[0][0]']       \n",
            "                                                                                                  \n",
            " res4b_branch2c (Conv2D)     (None, 14, 14, 1024)         263168    ['dropout_37[0][0]']          \n",
            "                                                                                                  \n",
            " bn4b_branch2c (BatchNormal  (None, 14, 14, 1024)         4096      ['res4b_branch2c[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_8 (Add)                 (None, 14, 14, 1024)         0         ['bn4b_branch2c[0][0]',       \n",
            "                                                                     'activation_57[0][0]']       \n",
            "                                                                                                  \n",
            " activation_60 (Activation)  (None, 14, 14, 1024)         0         ['add_8[0][0]']               \n",
            "                                                                                                  \n",
            " res4c_branch2a (Conv2D)     (None, 14, 14, 256)          262400    ['activation_60[0][0]']       \n",
            "                                                                                                  \n",
            " bn4c_branch2a (BatchNormal  (None, 14, 14, 256)          1024      ['res4c_branch2a[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_61 (Activation)  (None, 14, 14, 256)          0         ['bn4c_branch2a[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)        (None, 14, 14, 256)          0         ['activation_61[0][0]']       \n",
            "                                                                                                  \n",
            " res4c_branch2b (Conv2D)     (None, 14, 14, 256)          590080    ['dropout_38[0][0]']          \n",
            "                                                                                                  \n",
            " bn4c_branch2b (BatchNormal  (None, 14, 14, 256)          1024      ['res4c_branch2b[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_62 (Activation)  (None, 14, 14, 256)          0         ['bn4c_branch2b[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_39 (Dropout)        (None, 14, 14, 256)          0         ['activation_62[0][0]']       \n",
            "                                                                                                  \n",
            " res4c_branch2c (Conv2D)     (None, 14, 14, 1024)         263168    ['dropout_39[0][0]']          \n",
            "                                                                                                  \n",
            " bn4c_branch2c (BatchNormal  (None, 14, 14, 1024)         4096      ['res4c_branch2c[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_9 (Add)                 (None, 14, 14, 1024)         0         ['bn4c_branch2c[0][0]',       \n",
            "                                                                     'activation_60[0][0]']       \n",
            "                                                                                                  \n",
            " activation_63 (Activation)  (None, 14, 14, 1024)         0         ['add_9[0][0]']               \n",
            "                                                                                                  \n",
            " res4d_branch2a (Conv2D)     (None, 14, 14, 256)          262400    ['activation_63[0][0]']       \n",
            "                                                                                                  \n",
            " bn4d_branch2a (BatchNormal  (None, 14, 14, 256)          1024      ['res4d_branch2a[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_64 (Activation)  (None, 14, 14, 256)          0         ['bn4d_branch2a[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_40 (Dropout)        (None, 14, 14, 256)          0         ['activation_64[0][0]']       \n",
            "                                                                                                  \n",
            " res4d_branch2b (Conv2D)     (None, 14, 14, 256)          590080    ['dropout_40[0][0]']          \n",
            "                                                                                                  \n",
            " bn4d_branch2b (BatchNormal  (None, 14, 14, 256)          1024      ['res4d_branch2b[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_65 (Activation)  (None, 14, 14, 256)          0         ['bn4d_branch2b[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_41 (Dropout)        (None, 14, 14, 256)          0         ['activation_65[0][0]']       \n",
            "                                                                                                  \n",
            " res4d_branch2c (Conv2D)     (None, 14, 14, 1024)         263168    ['dropout_41[0][0]']          \n",
            "                                                                                                  \n",
            " bn4d_branch2c (BatchNormal  (None, 14, 14, 1024)         4096      ['res4d_branch2c[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_10 (Add)                (None, 14, 14, 1024)         0         ['bn4d_branch2c[0][0]',       \n",
            "                                                                     'activation_63[0][0]']       \n",
            "                                                                                                  \n",
            " activation_66 (Activation)  (None, 14, 14, 1024)         0         ['add_10[0][0]']              \n",
            "                                                                                                  \n",
            " res4e_branch2a (Conv2D)     (None, 14, 14, 256)          262400    ['activation_66[0][0]']       \n",
            "                                                                                                  \n",
            " bn4e_branch2a (BatchNormal  (None, 14, 14, 256)          1024      ['res4e_branch2a[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_67 (Activation)  (None, 14, 14, 256)          0         ['bn4e_branch2a[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_42 (Dropout)        (None, 14, 14, 256)          0         ['activation_67[0][0]']       \n",
            "                                                                                                  \n",
            " res4e_branch2b (Conv2D)     (None, 14, 14, 256)          590080    ['dropout_42[0][0]']          \n",
            "                                                                                                  \n",
            " bn4e_branch2b (BatchNormal  (None, 14, 14, 256)          1024      ['res4e_branch2b[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_68 (Activation)  (None, 14, 14, 256)          0         ['bn4e_branch2b[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_43 (Dropout)        (None, 14, 14, 256)          0         ['activation_68[0][0]']       \n",
            "                                                                                                  \n",
            " res4e_branch2c (Conv2D)     (None, 14, 14, 1024)         263168    ['dropout_43[0][0]']          \n",
            "                                                                                                  \n",
            " bn4e_branch2c (BatchNormal  (None, 14, 14, 1024)         4096      ['res4e_branch2c[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_11 (Add)                (None, 14, 14, 1024)         0         ['bn4e_branch2c[0][0]',       \n",
            "                                                                     'activation_66[0][0]']       \n",
            "                                                                                                  \n",
            " activation_69 (Activation)  (None, 14, 14, 1024)         0         ['add_11[0][0]']              \n",
            "                                                                                                  \n",
            " res4f_branch2a (Conv2D)     (None, 14, 14, 256)          262400    ['activation_69[0][0]']       \n",
            "                                                                                                  \n",
            " bn4f_branch2a (BatchNormal  (None, 14, 14, 256)          1024      ['res4f_branch2a[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_70 (Activation)  (None, 14, 14, 256)          0         ['bn4f_branch2a[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_44 (Dropout)        (None, 14, 14, 256)          0         ['activation_70[0][0]']       \n",
            "                                                                                                  \n",
            " res4f_branch2b (Conv2D)     (None, 14, 14, 256)          590080    ['dropout_44[0][0]']          \n",
            "                                                                                                  \n",
            " bn4f_branch2b (BatchNormal  (None, 14, 14, 256)          1024      ['res4f_branch2b[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_71 (Activation)  (None, 14, 14, 256)          0         ['bn4f_branch2b[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_45 (Dropout)        (None, 14, 14, 256)          0         ['activation_71[0][0]']       \n",
            "                                                                                                  \n",
            " res4f_branch2c (Conv2D)     (None, 14, 14, 1024)         263168    ['dropout_45[0][0]']          \n",
            "                                                                                                  \n",
            " bn4f_branch2c (BatchNormal  (None, 14, 14, 1024)         4096      ['res4f_branch2c[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_12 (Add)                (None, 14, 14, 1024)         0         ['bn4f_branch2c[0][0]',       \n",
            "                                                                     'activation_69[0][0]']       \n",
            "                                                                                                  \n",
            " activation_72 (Activation)  (None, 14, 14, 1024)         0         ['add_12[0][0]']              \n",
            "                                                                                                  \n",
            " res5a_branch2a (Conv2D)     (None, 7, 7, 512)            524800    ['activation_72[0][0]']       \n",
            "                                                                                                  \n",
            " bn5a_branch2a (BatchNormal  (None, 7, 7, 512)            2048      ['res5a_branch2a[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_73 (Activation)  (None, 7, 7, 512)            0         ['bn5a_branch2a[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_46 (Dropout)        (None, 7, 7, 512)            0         ['activation_73[0][0]']       \n",
            "                                                                                                  \n",
            " res5a_branch2b (Conv2D)     (None, 7, 7, 512)            2359808   ['dropout_46[0][0]']          \n",
            "                                                                                                  \n",
            " bn5a_branch2b (BatchNormal  (None, 7, 7, 512)            2048      ['res5a_branch2b[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_74 (Activation)  (None, 7, 7, 512)            0         ['bn5a_branch2b[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_47 (Dropout)        (None, 7, 7, 512)            0         ['activation_74[0][0]']       \n",
            "                                                                                                  \n",
            " res5a_branch2c (Conv2D)     (None, 7, 7, 2048)           1050624   ['dropout_47[0][0]']          \n",
            "                                                                                                  \n",
            " res5a_branch1 (Conv2D)      (None, 7, 7, 2048)           2099200   ['activation_72[0][0]']       \n",
            "                                                                                                  \n",
            " bn5a_branch2c (BatchNormal  (None, 7, 7, 2048)           8192      ['res5a_branch2c[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " bn5a_branch1 (BatchNormali  (None, 7, 7, 2048)           8192      ['res5a_branch1[0][0]']       \n",
            " zation)                                                                                          \n",
            "                                                                                                  \n",
            " add_13 (Add)                (None, 7, 7, 2048)           0         ['bn5a_branch2c[0][0]',       \n",
            "                                                                     'bn5a_branch1[0][0]']        \n",
            "                                                                                                  \n",
            " activation_75 (Activation)  (None, 7, 7, 2048)           0         ['add_13[0][0]']              \n",
            "                                                                                                  \n",
            " res5b_branch2a (Conv2D)     (None, 7, 7, 512)            1049088   ['activation_75[0][0]']       \n",
            "                                                                                                  \n",
            " bn5b_branch2a (BatchNormal  (None, 7, 7, 512)            2048      ['res5b_branch2a[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_76 (Activation)  (None, 7, 7, 512)            0         ['bn5b_branch2a[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_48 (Dropout)        (None, 7, 7, 512)            0         ['activation_76[0][0]']       \n",
            "                                                                                                  \n",
            " res5b_branch2b (Conv2D)     (None, 7, 7, 512)            2359808   ['dropout_48[0][0]']          \n",
            "                                                                                                  \n",
            " bn5b_branch2b (BatchNormal  (None, 7, 7, 512)            2048      ['res5b_branch2b[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_77 (Activation)  (None, 7, 7, 512)            0         ['bn5b_branch2b[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_49 (Dropout)        (None, 7, 7, 512)            0         ['activation_77[0][0]']       \n",
            "                                                                                                  \n",
            " res5b_branch2c (Conv2D)     (None, 7, 7, 2048)           1050624   ['dropout_49[0][0]']          \n",
            "                                                                                                  \n",
            " bn5b_branch2c (BatchNormal  (None, 7, 7, 2048)           8192      ['res5b_branch2c[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_14 (Add)                (None, 7, 7, 2048)           0         ['bn5b_branch2c[0][0]',       \n",
            "                                                                     'activation_75[0][0]']       \n",
            "                                                                                                  \n",
            " activation_78 (Activation)  (None, 7, 7, 2048)           0         ['add_14[0][0]']              \n",
            "                                                                                                  \n",
            " res5c_branch2a (Conv2D)     (None, 7, 7, 512)            1049088   ['activation_78[0][0]']       \n",
            "                                                                                                  \n",
            " bn5c_branch2a (BatchNormal  (None, 7, 7, 512)            2048      ['res5c_branch2a[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_79 (Activation)  (None, 7, 7, 512)            0         ['bn5c_branch2a[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_50 (Dropout)        (None, 7, 7, 512)            0         ['activation_79[0][0]']       \n",
            "                                                                                                  \n",
            " res5c_branch2b (Conv2D)     (None, 7, 7, 512)            2359808   ['dropout_50[0][0]']          \n",
            "                                                                                                  \n",
            " bn5c_branch2b (BatchNormal  (None, 7, 7, 512)            2048      ['res5c_branch2b[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " activation_80 (Activation)  (None, 7, 7, 512)            0         ['bn5c_branch2b[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_51 (Dropout)        (None, 7, 7, 512)            0         ['activation_80[0][0]']       \n",
            "                                                                                                  \n",
            " res5c_branch2c (Conv2D)     (None, 7, 7, 2048)           1050624   ['dropout_51[0][0]']          \n",
            "                                                                                                  \n",
            " bn5c_branch2c (BatchNormal  (None, 7, 7, 2048)           8192      ['res5c_branch2c[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_15 (Add)                (None, 7, 7, 2048)           0         ['bn5c_branch2c[0][0]',       \n",
            "                                                                     'activation_78[0][0]']       \n",
            "                                                                                                  \n",
            " activation_81 (Activation)  (None, 7, 7, 2048)           0         ['add_15[0][0]']              \n",
            "                                                                                                  \n",
            " feature_layer (AveragePool  (None, 3, 3, 2048)           0         ['activation_81[0][0]']       \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)         (None, 18432)                0         ['feature_layer[0][0]']       \n",
            "                                                                                                  \n",
            " fc2 (Dense)                 (None, 2)                    36866     ['flatten_3[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23618306 (90.10 MB)\n",
            "Trainable params: 23565186 (89.89 MB)\n",
            "Non-trainable params: 53120 (207.50 KB)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/16\n",
            "23/23 [==============================] - ETA: 0s - loss: 3.1985 - accuracy: 0.6543"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 16 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r23/23 [==============================] - 70s 2s/step - loss: 3.1985 - accuracy: 0.6543 - val_loss: 0.6574 - val_accuracy: 0.7933\n",
            "Epoch 2/16\n",
            "23/23 [==============================] - 45s 2s/step - loss: 1.6976 - accuracy: 0.6777\n",
            "Epoch 3/16\n",
            "23/23 [==============================] - 44s 2s/step - loss: 2.0562 - accuracy: 0.6928\n",
            "Epoch 4/16\n",
            "23/23 [==============================] - 44s 2s/step - loss: 1.1805 - accuracy: 0.6639\n",
            "Epoch 5/16\n",
            "23/23 [==============================] - 44s 2s/step - loss: 0.9543 - accuracy: 0.7300\n",
            "Epoch 6/16\n",
            "23/23 [==============================] - 44s 2s/step - loss: 0.6372 - accuracy: 0.7507\n",
            "Epoch 7/16\n",
            "23/23 [==============================] - 44s 2s/step - loss: 0.6139 - accuracy: 0.7548\n",
            "Epoch 8/16\n",
            "23/23 [==============================] - 44s 2s/step - loss: 0.7152 - accuracy: 0.7521\n",
            "Epoch 9/16\n",
            "23/23 [==============================] - 44s 2s/step - loss: 1.0067 - accuracy: 0.7452\n",
            "Epoch 10/16\n",
            "23/23 [==============================] - 44s 2s/step - loss: 0.9868 - accuracy: 0.7149\n",
            "Epoch 11/16\n",
            "23/23 [==============================] - 44s 2s/step - loss: 0.7628 - accuracy: 0.7410\n",
            "Epoch 12/16\n",
            "23/23 [==============================] - 43s 2s/step - loss: 0.9634 - accuracy: 0.7135\n",
            "Epoch 13/16\n",
            "23/23 [==============================] - 44s 2s/step - loss: 0.8271 - accuracy: 0.6791\n",
            "Epoch 14/16\n",
            "23/23 [==============================] - 44s 2s/step - loss: 1.0176 - accuracy: 0.6928\n",
            "Epoch 15/16\n",
            "23/23 [==============================] - 44s 2s/step - loss: 0.6513 - accuracy: 0.7397\n",
            "Epoch 16/16\n",
            "23/23 [==============================] - 44s 2s/step - loss: 0.6778 - accuracy: 0.7493\n",
            "[INFO] Calculating model accuracy\n",
            "12/12 [==============================] - 6s 464ms/step - loss: 0.5705 - accuracy: 0.7933\n",
            "Test Accuracy: 79.32960987091064\n",
            "12/12 [==============================] - 6s 468ms/step - loss: 0.5705 - accuracy: 0.7933\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.5876 - accuracy: 0.7603\n",
            "Train Accuracy: 76.03305578231812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Assuming `model` is your trained model\n",
        "joblib.dump(model_resnet, '/content/drive/My Drive/model_resnet.pkl')  # This will overwrite any existing file at that path.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1gJ6_qxnp9u",
        "outputId": "c3f824cf-9afb-4ef0-90e5-06ebf080047a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/model_resnet.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_densenet.save(\"densenet.h5\")\n",
        "model_cnn.save(\"cnn.h5\")\n",
        "model_resnet.save(\"resnet.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbZD0Yyjq026",
        "outputId": "7e5e44aa-1de2-4283-e925-067432aa9a61"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_densenet=keras.models.load_model(\"densenet.h5\")\n",
        "model_cnn=keras.models.load_model(\"cnn.h5\")\n",
        "model_resnet=keras.models.load_model(\"resnet.h5\")"
      ],
      "metadata": {
        "id": "H1sths0buQdR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "utpd3h9PzaKj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iGj5fX7Dzmx1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def extract(model,x_train,x_test,y_train,y_test,layer):\n",
        "  new_model=Model(inputs=model.input,outputs=model.get_layer(layer).output)\n",
        "\n",
        "  #Let's obtain the Input Representations\n",
        "  x_train_n=new_model.predict(x_train)\n",
        "  x_test_n=new_model.predict(x_test)\n",
        "\n",
        "  #Convert back the labels\n",
        "  y_train_n=[ np.where(r==1)[0][0] for r in y_train ]\n",
        "  y_test_n=[ np.where(r==1)[0][0] for r in y_test ]\n",
        "  features=x_train_n.shape[1]*x_train_n.shape[2]*x_train_n.shape[3]\n",
        "  x_train_new = np.reshape(x_train_n, (-1, features))\n",
        "  x_test_new = np.reshape(x_test_n, (-1, features))\n",
        "\n",
        "  x_train=x_train_new\n",
        "  x_test=x_test_new\n",
        "  y_train=y_train_n\n",
        "  y_test=y_test_n\n",
        "\n",
        "  return x_train,x_test,y_train,y_test\n",
        "\n",
        "\"\"\"#Feature Extraction from DL models\n",
        "\n",
        "# For DenseNet\"\"\"\n",
        "x_train_densenet, x_test_densenet, y_train_densenet, y_test_densenet = extract(model_densenet, x_train, x_test, y_train, y_test, 'global_average_pooling2d_2')\n",
        "\n",
        "# For CNN\n",
        "x_train_cnn, x_test_cnn, y_train_cnn, y_test_cnn = extract(model_cnn, x_train, x_test, y_train, y_test, 'feature_layer')\n",
        "\n",
        "# For ResNet\n",
        "x_train_resnet, x_test_resnet, y_train_resnet, y_test_resnet = extract(model_resnet, x_train, x_test, y_train, y_test, 'feature_layer')\n",
        "\n",
        "'''\n",
        "def extract(model, x_train, x_test, y_train, y_test, layer):\n",
        "    new_model = Model(inputs=model.input, outputs=model.get_layer(layer).output)\n",
        "\n",
        "    # Obtain the Input Representations\n",
        "    x_train_n = new_model.predict(x_train)\n",
        "    x_test_n = new_model.predict(x_test)\n",
        "\n",
        "    # Check output shapes\n",
        "    print(\"x_train_n shape:\", x_train_n.shape)\n",
        "    print(\"x_test_n shape:\", x_test_n.shape)\n",
        "\n",
        "    # Convert back the labels\n",
        "    y_train_n = [np.where(r == 1)[0][0] for r in y_train]\n",
        "    y_test_n = [np.where(r == 1)[0][0] for r in y_test]\n",
        "\n",
        "    # Reshape according to the output dimensions\n",
        "    if len(x_train_n.shape) == 2:  # Handle 2D output (e.g., (samples, features))\n",
        "        x_train_new = x_train_n  # No reshape needed if already 2D\n",
        "        x_test_new = x_test_n\n",
        "    elif len(x_train_n.shape) == 4:  # Handle 4D output\n",
        "        features = x_train_n.shape[1] * x_train_n.shape[2] * x_train_n.shape[3]\n",
        "        x_train_new = np.reshape(x_train_n, (-1, features))\n",
        "        x_test_new = np.reshape(x_test_n, (-1, features))\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected output shape from the layer.\")\n",
        "\n",
        "    return x_train_new, x_test_new, y_train_n, y_test_n\n"
      ],
      "metadata": {
        "id": "pai9h4SxPMRN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For DenseNet\"\"\"\n",
        "x_train_densenet, x_test_densenet, y_train_densenet, y_test_densenet = extract(model_densenet, x_train, x_test, y_train, y_test, 'global_average_pooling2d_1')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhrrsqMc8LfM",
        "outputId": "e0bbe375-ce5d-47bb-eb7f-f3aa600a497e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/23 [==============================] - 4s 164ms/step\n",
            "12/12 [==============================] - 2s 153ms/step\n",
            "x_train_n shape: (726, 72)\n",
            "x_test_n shape: (358, 72)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For CNN\n",
        "x_train_cnn, x_test_cnn, y_train_cnn, y_test_cnn = extract(model_cnn, x_train, x_test, y_train, y_test, 'feature_layer')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8TuJZGU8NZ7",
        "outputId": "36604e66-1eb2-45b9-cbab-a8e70784287a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/23 [==============================] - 3s 106ms/step\n",
            "12/12 [==============================] - 1s 97ms/step\n",
            "x_train_n shape: (726, 18, 18, 128)\n",
            "x_test_n shape: (358, 18, 18, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# For ResNet\n",
        "x_train_resnet, x_test_resnet, y_train_resnet, y_test_resnet = extract(model_resnet, x_train, x_test, y_train, y_test, 'feature_layer')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBmTfHib8O_r",
        "outputId": "756b7593-4b14-4c75-d4dc-77ad2427acca"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/23 [==============================] - 12s 455ms/step\n",
            "12/12 [==============================] - 5s 421ms/step\n",
            "x_train_n shape: (726, 3, 3, 2048)\n",
            "x_test_n shape: (358, 3, 3, 2048)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''def pca(x_train,x_test,model):\n",
        "  if model=='densenet':\n",
        "    components=361\n",
        "  elif model=='cnn':\n",
        "    components=256\n",
        "  elif model=='vgg16':\n",
        "    components=256\n",
        "\n",
        "  pca_model = PCA(n_components=components)\n",
        "  pca_model.fit(x_train)\n",
        "  train_images_reduced = pca_model.transform(x_train)\n",
        "  test_images_reduced = pca_model.transform(x_test)\n",
        "\n",
        "  # verify shape after PCA\n",
        "  print(\"Train images shape:\", train_images_reduced.shape)\n",
        "  print(\"Test images shape:\", test_images_reduced.shape)\n",
        "\n",
        "  # get exact variability retained\n",
        "  print(\"\\nVar retained (%):\",\n",
        "        np.sum(pca_model.explained_variance_ratio_ * 100))\n",
        "\n",
        "  x_train=train_images_reduced\n",
        "  x_test=test_images_reduced\n",
        "\n",
        "  return x_train,x_test'''\n",
        "\"\"\"def pca(x_train, x_test, model):\n",
        "    # Determine components based on model or shape of data\n",
        "    if model == 'densenet':\n",
        "        components = min(x_train.shape[0], x_train.shape[1], 361)  # Adjust based on shape\n",
        "    elif model == 'cnn':\n",
        "        components = min(x_train.shape[0], x_train.shape[1], 256)\n",
        "    elif model == 'resnet':\n",
        "        components = min(x_train.shape[0], x_train.shape[1], 256)\n",
        "\n",
        "    # Apply PCA\n",
        "    pca_model = PCA(n_components=components)\n",
        "    pca_model.fit(x_train)\n",
        "    train_images_reduced = pca_model.transform(x_train)\n",
        "    test_images_reduced = pca_model.transform(x_test)\n",
        "\n",
        "    # Check variability retained\n",
        "    print(\"Train images shape:\", train_images_reduced.shape)\n",
        "    print(\"Test images shape:\", test_images_reduced.shape)\n",
        "    print(\"\\nVar retained (%):\", np.sum(pca_model.explained_variance_ratio_ * 100))\n",
        "\n",
        "    return train_images_reduced, test_images_reduced\"\"\"\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def pca(x_train, x_test, model):\n",
        "    # Determine the number of components based on the model type\n",
        "    if model == 'densenet':\n",
        "        components = min(x_train.shape[0], x_train.shape[1], 72)  # Adjust to 72 features as per your requirement\n",
        "    elif model == 'cnn':\n",
        "        components = min(x_train.shape[0], x_train.shape[1], 256)\n",
        "    elif model == 'resnet':\n",
        "        components = min(x_train.shape[0], x_train.shape[1], 256)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model type specified\")\n",
        "\n",
        "    # Apply PCA\n",
        "    pca_model = PCA(n_components=components)\n",
        "    pca_model.fit(x_train)  # Fit PCA on the training data\n",
        "\n",
        "    # Transform the datasets\n",
        "    train_images_reduced = pca_model.transform(x_train)\n",
        "    test_images_reduced = pca_model.transform(x_test)\n",
        "\n",
        "    # Check variability retained\n",
        "    print(\"Train images shape:\", train_images_reduced.shape)\n",
        "    print(\"Test images shape:\", test_images_reduced.shape)\n",
        "    print(\"\\nVariance retained (%):\", np.sum(pca_model.explained_variance_ratio_ * 100))\n",
        "\n",
        "    return train_images_reduced, test_images_reduced\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "amGZA_sysJ2I"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# For DenseNet\n",
        "x_train_red_densenet, x_test_red_densenet = pca(x_train_densenet, x_test_densenet, \"densenet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pEpw8EM4pPv",
        "outputId": "413458de-a9e0-439e-c19a-6f92d90c2707"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images shape: (726, 72)\n",
            "Test images shape: (358, 72)\n",
            "\n",
            "Variance retained (%): 100.000015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For CNN\n",
        "x_train_red_cnn, x_test_red_cnn = pca(x_train_cnn, x_test_cnn, \"cnn\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssvFFuQi4k6F",
        "outputId": "3ddd51a1-92ac-4095-9b61-7486af2c0fa0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images shape: (726, 256)\n",
            "Test images shape: (358, 256)\n",
            "\n",
            "Variance retained (%): 95.37291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For ResNet\n",
        "x_train_red_resnet, x_test_red_resnet = pca(x_train_resnet, x_test_resnet, \"resnet\")  # Replace with 'resnet' if needed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfHfhxfx4mzd",
        "outputId": "ca02da2c-0868-4cfe-99d5-b95b3a5a61f1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images shape: (726, 256)\n",
            "Test images shape: (358, 256)\n",
            "\n",
            "Variance retained (%): 99.3041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"x_train_red_densenet,x_test_red_densenet=pca(x_train_densenet,x_test_densenet,densenet)\n",
        "\n",
        "\n",
        "\n",
        "x_train_red_cnn,x_test_red_cnn=pca(x_train_cnn,x_test_cnn,cnn)\n",
        "\n",
        "\n",
        "\n",
        "x_train_red_resnet,x_test_red_resnet=pca(x_train_resnet,x_test_resnet,resnet)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def display_metrics(a,b):\n",
        "  conf_matrix = confusion_matrix(a,b)\n",
        "  FP = conf_matrix[0,1]\n",
        "  FN = conf_matrix[1,0]\n",
        "  TP = conf_matrix[0,0]\n",
        "  TN = conf_matrix[1,1]\n",
        "\n",
        "  # Sensitivity, hit rate, recall, or true positive rate\n",
        "  print(\"Recall/ Sensitivity: \",TP/(TP+FN))\n",
        "  # Specificity or true negative rate\n",
        "  print(\"Specificity: \",TN/(TN+FP))\n",
        "  # Precision or positive predictive value\n",
        "  print(\"Precision: \",TP/(TP+FP))\n",
        "  # Fall out or false positive rate\n",
        "  print(\"False positive rate: \",FP/(FP+TN))\n",
        "  # Overall accuracy\n",
        "  print(\"Accuracy: \",(TP+TN)/(TP+FP+FN+TN) *100)\n",
        "  print(\"Classification Report: \")\n",
        "  print(classification_report(a,b))"
      ],
      "metadata": {
        "id": "AJLgm0Pm3ZEK"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NWu31Ty23iDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\"\"\"#Define Machine Learning classifiers\n",
        "\n",
        "**K-Nearest Neighbor**\n",
        "\"\"\"\n",
        "\n",
        "def knn_ml(x_train,x_test,y_train,y_test):\n",
        "  from sklearn.neighbors import KNeighborsClassifier\n",
        "  knc=KNeighborsClassifier()\n",
        "  knc.fit(x_train,y_train)\n",
        "  knc.score(x_train,y_train)\n",
        "  knn_y_pred=knc.predict(x_test)\n",
        "  knn_x_pred=knc.predict(x_train)\n",
        "\n",
        "  from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
        "  print(\"Training Data : \")\n",
        "  display_metrics(y_train,knn_x_pred)\n",
        "  print(\"Testing Data : \")\n",
        "  display_metrics(y_test,knn_y_pred)\n",
        "\n",
        "  return knc\n",
        "\n",
        "\"\"\"**Support Vector Machine**\"\"\"\n",
        "\n",
        "def svm_ml(x_train,x_test,y_train,y_test):\n",
        "  from sklearn.svm import SVC\n",
        "  svm = SVC(kernel='linear')\n",
        "  svm.fit(x_train, y_train)\n",
        "  svm_y_pred = svm.predict(x_test)\n",
        "  svm_x_pred = svm.predict(x_train)\n",
        "\n",
        "  from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
        "  print(\"Training Data : \")\n",
        "  display_metrics(y_train,svm_x_pred)\n",
        "  print(\"Testing Data : \")\n",
        "  display_metrics(y_test,svm_y_pred)\n",
        "\n",
        "  return svm\n",
        "\n",
        "\"\"\"**Naive Bayes**\"\"\"\n",
        "\n",
        "def naivebayes_ml(x_train,x_test,y_train,y_test):\n",
        "  from sklearn.naive_bayes import GaussianNB\n",
        "  nb = GaussianNB()\n",
        "  nb.fit(x_train, y_train)\n",
        "  nb_y_pred=nb.predict(x_test)\n",
        "  nb_x_pred=nb.predict(x_train)\n",
        "\n",
        "  from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
        "  print(\"Training Data : \")\n",
        "  display_metrics(y_train,nb_x_pred)\n",
        "  print(\"Testing Data : \")\n",
        "  display_metrics(y_test,nb_y_pred)\n",
        "\n",
        "  return nb\n",
        "\n",
        "\"\"\"**Logistic Regression**\"\"\"\n",
        "\n",
        "def logreg_ml(x_train,x_test,y_train,y_test):\n",
        "  from sklearn.linear_model import LogisticRegression\n",
        "  lr = LogisticRegression(solver='saga')\n",
        "  lr.fit(x_train, y_train)\n",
        "  lr_y_pred=lr.predict(x_test)\n",
        "  lr_x_pred=lr.predict(x_train)\n",
        "\n",
        "  from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
        "  print(\"Training Data : \")\n",
        "  display_metrics(y_train,lr_x_pred)\n",
        "  print(\"Testing Data : \")\n",
        "  display_metrics(y_test,lr_y_pred)\n",
        "\n",
        "  return lr\n",
        "\n",
        "\"\"\"#Evaluate ML classifiers using DL extracted, reduced features\n",
        "\n",
        "**DenseNet**\n",
        "\"\"\"\n",
        "\n",
        "densenet_knn = knn_ml(x_train_red_densenet,x_test_red_densenet,y_train_densenet,y_test_densenet)\n",
        "\n",
        "densenet_svm = svm_ml(x_train_red_densenet,x_test_red_densenet,y_train_densenet,y_test_densenet)\n",
        "\n",
        "densenet_nb = naivebayes_ml(x_train_red_densenet,x_test_red_densenet,y_train_densenet,y_test_densenet)\n",
        "\n",
        "densenet_lr = logreg_ml(x_train_red_densenet,x_test_red_densenet,y_train_densenet,y_test_densenet)\n",
        "\n",
        "\"\"\"**CNN**\"\"\"\n",
        "\n",
        "cnn_knn = knn_ml(x_train_red_cnn,x_test_red_cnn,y_train_cnn,y_test_cnn)\n",
        "\n",
        "cnn_svm = svm_ml(x_train_red_cnn,x_test_red_cnn,y_train_cnn,y_test_cnn)\n",
        "\n",
        "cnn_nb = naivebayes_ml(x_train_red_cnn,x_test_red_cnn,y_train_cnn,y_test_cnn)\n",
        "\n",
        "cnn_lr = logreg_ml(x_train_red_cnn,x_test_red_cnn,y_train_cnn,y_test_cnn)\n",
        "\n",
        "\"\"\"**Resnet**\"\"\"\n",
        "\n",
        "resnet_knn = knn_ml(x_train_red_resnet,x_test_red_resnet,y_train_resnet,y_test_resnet)\n",
        "\n",
        "resnet_svm = svm_ml(x_train_red_resnet,x_test_red_resnet,y_train_resnet,y_test_resnet)\n",
        "\n",
        "resnet_nb = naivebayes_ml(x_train_red_resnet,x_test_red_resnet,y_train_resnet,y_test_resnet)\n",
        "\n",
        "resnet_lr = logreg_ml(x_train_red_resnet,x_test_red_resnet,y_train_resnet,y_test_resnet)\n",
        "\n",
        "\"\"\"#Graphical User Interface\"\"\"\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_kHr7sWuCld8",
        "outputId": "76a90978-4f23-4257-fcf9-8d3f0ef9dd7b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data : \n",
            "Recall/ Sensitivity:  0.8656716417910447\n",
            "Specificity:  0.7560975609756098\n",
            "Precision:  0.9456521739130435\n",
            "False positive rate:  0.24390243902439024\n",
            "Accuracy:  84.71074380165288\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.95      0.90       552\n",
            "           1       0.76      0.53      0.63       174\n",
            "\n",
            "    accuracy                           0.85       726\n",
            "   macro avg       0.81      0.74      0.77       726\n",
            "weighted avg       0.84      0.85      0.84       726\n",
            "\n",
            "Testing Data : \n",
            "Recall/ Sensitivity:  0.8344155844155844\n",
            "Specificity:  0.46\n",
            "Precision:  0.9049295774647887\n",
            "False positive rate:  0.54\n",
            "Accuracy:  78.2122905027933\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.87       284\n",
            "           1       0.46      0.31      0.37        74\n",
            "\n",
            "    accuracy                           0.78       358\n",
            "   macro avg       0.65      0.61      0.62       358\n",
            "weighted avg       0.76      0.78      0.77       358\n",
            "\n",
            "Training Data : \n",
            "Recall/ Sensitivity:  0.7603305785123967\n",
            "Specificity:  nan\n",
            "Precision:  1.0\n",
            "False positive rate:  nan\n",
            "Accuracy:  76.03305785123968\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      1.00      0.86       552\n",
            "           1       0.00      0.00      0.00       174\n",
            "\n",
            "    accuracy                           0.76       726\n",
            "   macro avg       0.38      0.50      0.43       726\n",
            "weighted avg       0.58      0.76      0.66       726\n",
            "\n",
            "Testing Data : \n",
            "Recall/ Sensitivity:  0.7932960893854749\n",
            "Specificity:  nan\n",
            "Precision:  1.0\n",
            "False positive rate:  nan\n",
            "Accuracy:  79.3296089385475\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      1.00      0.88       284\n",
            "           1       0.00      0.00      0.00        74\n",
            "\n",
            "    accuracy                           0.79       358\n",
            "   macro avg       0.40      0.50      0.44       358\n",
            "weighted avg       0.63      0.79      0.70       358\n",
            "\n",
            "Training Data : \n",
            "Recall/ Sensitivity:  0.872113676731794\n",
            "Specificity:  0.6257668711656442\n",
            "Precision:  0.8894927536231884\n",
            "False positive rate:  0.37423312883435583\n",
            "Accuracy:  81.68044077134986\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.89      0.88       552\n",
            "           1       0.63      0.59      0.61       174\n",
            "\n",
            "    accuracy                           0.82       726\n",
            "   macro avg       0.75      0.74      0.74       726\n",
            "weighted avg       0.81      0.82      0.81       726\n",
            "\n",
            "Testing Data : \n",
            "Recall/ Sensitivity:  0.852233676975945\n",
            "Specificity:  0.4626865671641791\n",
            "Precision:  0.8732394366197183\n",
            "False positive rate:  0.5373134328358209\n",
            "Accuracy:  77.93296089385476\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       284\n",
            "           1       0.46      0.42      0.44        74\n",
            "\n",
            "    accuracy                           0.78       358\n",
            "   macro avg       0.66      0.65      0.65       358\n",
            "weighted avg       0.77      0.78      0.78       358\n",
            "\n",
            "Training Data : \n",
            "Recall/ Sensitivity:  0.7624309392265194\n",
            "Specificity:  1.0\n",
            "Precision:  1.0\n",
            "False positive rate:  0.0\n",
            "Accuracy:  76.30853994490359\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      1.00      0.87       552\n",
            "           1       1.00      0.01      0.02       174\n",
            "\n",
            "    accuracy                           0.76       726\n",
            "   macro avg       0.88      0.51      0.44       726\n",
            "weighted avg       0.82      0.76      0.66       726\n",
            "\n",
            "Testing Data : \n",
            "Recall/ Sensitivity:  0.7932960893854749\n",
            "Specificity:  nan\n",
            "Precision:  1.0\n",
            "False positive rate:  nan\n",
            "Accuracy:  79.3296089385475\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      1.00      0.88       284\n",
            "           1       0.00      0.00      0.00        74\n",
            "\n",
            "    accuracy                           0.79       358\n",
            "   macro avg       0.40      0.50      0.44       358\n",
            "weighted avg       0.63      0.79      0.70       358\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-37c5e1d8e47e>:23: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  print(\"Specificity: \",TN/(TN+FP))\n",
            "<ipython-input-32-37c5e1d8e47e>:27: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  print(\"False positive rate: \",FP/(FP+TN))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "<ipython-input-32-37c5e1d8e47e>:23: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  print(\"Specificity: \",TN/(TN+FP))\n",
            "<ipython-input-32-37c5e1d8e47e>:27: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  print(\"False positive rate: \",FP/(FP+TN))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "<ipython-input-32-37c5e1d8e47e>:23: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  print(\"Specificity: \",TN/(TN+FP))\n",
            "<ipython-input-32-37c5e1d8e47e>:27: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  print(\"False positive rate: \",FP/(FP+TN))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data : \n",
            "Recall/ Sensitivity:  0.8202416918429003\n",
            "Specificity:  0.859375\n",
            "Precision:  0.9836956521739131\n",
            "False positive rate:  0.140625\n",
            "Accuracy:  82.36914600550963\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.98      0.89       552\n",
            "           1       0.86      0.32      0.46       174\n",
            "\n",
            "    accuracy                           0.82       726\n",
            "   macro avg       0.84      0.65      0.68       726\n",
            "weighted avg       0.83      0.82      0.79       726\n",
            "\n",
            "Testing Data : \n",
            "Recall/ Sensitivity:  0.8128654970760234\n",
            "Specificity:  0.625\n",
            "Precision:  0.9788732394366197\n",
            "False positive rate:  0.375\n",
            "Accuracy:  80.44692737430168\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.98      0.89       284\n",
            "           1       0.62      0.14      0.22        74\n",
            "\n",
            "    accuracy                           0.80       358\n",
            "   macro avg       0.72      0.56      0.56       358\n",
            "weighted avg       0.77      0.80      0.75       358\n",
            "\n",
            "Training Data : \n",
            "Recall/ Sensitivity:  1.0\n",
            "Specificity:  1.0\n",
            "Precision:  1.0\n",
            "False positive rate:  0.0\n",
            "Accuracy:  100.0\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       552\n",
            "           1       1.00      1.00      1.00       174\n",
            "\n",
            "    accuracy                           1.00       726\n",
            "   macro avg       1.00      1.00      1.00       726\n",
            "weighted avg       1.00      1.00      1.00       726\n",
            "\n",
            "Testing Data : \n",
            "Recall/ Sensitivity:  0.8339483394833949\n",
            "Specificity:  0.3333333333333333\n",
            "Precision:  0.795774647887324\n",
            "False positive rate:  0.6666666666666666\n",
            "Accuracy:  71.22905027932961\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.80      0.81       284\n",
            "           1       0.33      0.39      0.36        74\n",
            "\n",
            "    accuracy                           0.71       358\n",
            "   macro avg       0.58      0.59      0.59       358\n",
            "weighted avg       0.73      0.71      0.72       358\n",
            "\n",
            "Training Data : \n",
            "Recall/ Sensitivity:  0.8729281767955801\n",
            "Specificity:  0.5737704918032787\n",
            "Precision:  0.8586956521739131\n",
            "False positive rate:  0.4262295081967213\n",
            "Accuracy:  79.75206611570248\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.86      0.87       552\n",
            "           1       0.57      0.60      0.59       174\n",
            "\n",
            "    accuracy                           0.80       726\n",
            "   macro avg       0.72      0.73      0.73       726\n",
            "weighted avg       0.80      0.80      0.80       726\n",
            "\n",
            "Testing Data : \n",
            "Recall/ Sensitivity:  0.8238993710691824\n",
            "Specificity:  0.45\n",
            "Precision:  0.9225352112676056\n",
            "False positive rate:  0.55\n",
            "Accuracy:  78.2122905027933\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.92      0.87       284\n",
            "           1       0.45      0.24      0.32        74\n",
            "\n",
            "    accuracy                           0.78       358\n",
            "   macro avg       0.64      0.58      0.59       358\n",
            "weighted avg       0.75      0.78      0.76       358\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data : \n",
            "Recall/ Sensitivity:  0.974903474903475\n",
            "Specificity:  0.7740384615384616\n",
            "Precision:  0.9148550724637681\n",
            "False positive rate:  0.22596153846153846\n",
            "Accuracy:  91.73553719008265\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.91      0.94       552\n",
            "           1       0.77      0.93      0.84       174\n",
            "\n",
            "    accuracy                           0.92       726\n",
            "   macro avg       0.87      0.92      0.89       726\n",
            "weighted avg       0.93      0.92      0.92       726\n",
            "\n",
            "Testing Data : \n",
            "Recall/ Sensitivity:  0.864\n",
            "Specificity:  0.37037037037037035\n",
            "Precision:  0.7605633802816901\n",
            "False positive rate:  0.6296296296296297\n",
            "Accuracy:  71.50837988826815\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.76      0.81       284\n",
            "           1       0.37      0.54      0.44        74\n",
            "\n",
            "    accuracy                           0.72       358\n",
            "   macro avg       0.62      0.65      0.62       358\n",
            "weighted avg       0.76      0.72      0.73       358\n",
            "\n",
            "Training Data : \n",
            "Recall/ Sensitivity:  0.8496835443037974\n",
            "Specificity:  0.8404255319148937\n",
            "Precision:  0.9728260869565217\n",
            "False positive rate:  0.1595744680851064\n",
            "Accuracy:  84.84848484848484\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.97      0.91       552\n",
            "           1       0.84      0.45      0.59       174\n",
            "\n",
            "    accuracy                           0.85       726\n",
            "   macro avg       0.85      0.71      0.75       726\n",
            "weighted avg       0.85      0.85      0.83       726\n",
            "\n",
            "Testing Data : \n",
            "Recall/ Sensitivity:  0.824773413897281\n",
            "Specificity:  0.5925925925925926\n",
            "Precision:  0.9612676056338029\n",
            "False positive rate:  0.4074074074074074\n",
            "Accuracy:  80.72625698324022\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.96      0.89       284\n",
            "           1       0.59      0.22      0.32        74\n",
            "\n",
            "    accuracy                           0.81       358\n",
            "   macro avg       0.71      0.59      0.60       358\n",
            "weighted avg       0.78      0.81      0.77       358\n",
            "\n",
            "Training Data : \n",
            "Recall/ Sensitivity:  0.7603305785123967\n",
            "Specificity:  nan\n",
            "Precision:  1.0\n",
            "False positive rate:  nan\n",
            "Accuracy:  76.03305785123968\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      1.00      0.86       552\n",
            "           1       0.00      0.00      0.00       174\n",
            "\n",
            "    accuracy                           0.76       726\n",
            "   macro avg       0.38      0.50      0.43       726\n",
            "weighted avg       0.58      0.76      0.66       726\n",
            "\n",
            "Testing Data : \n",
            "Recall/ Sensitivity:  0.7932960893854749\n",
            "Specificity:  nan\n",
            "Precision:  1.0\n",
            "False positive rate:  nan\n",
            "Accuracy:  79.3296089385475\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      1.00      0.88       284\n",
            "           1       0.00      0.00      0.00        74\n",
            "\n",
            "    accuracy                           0.79       358\n",
            "   macro avg       0.40      0.50      0.44       358\n",
            "weighted avg       0.63      0.79      0.70       358\n",
            "\n",
            "Training Data : \n",
            "Recall/ Sensitivity:  0.8904109589041096\n",
            "Specificity:  0.7746478873239436\n",
            "Precision:  0.9420289855072463\n",
            "False positive rate:  0.22535211267605634\n",
            "Accuracy:  86.77685950413223\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.94      0.92       552\n",
            "           1       0.77      0.63      0.70       174\n",
            "\n",
            "    accuracy                           0.87       726\n",
            "   macro avg       0.83      0.79      0.81       726\n",
            "weighted avg       0.86      0.87      0.86       726\n",
            "\n",
            "Testing Data : \n",
            "Recall/ Sensitivity:  0.8370607028753994\n",
            "Specificity:  0.5111111111111111\n",
            "Precision:  0.9225352112676056\n",
            "False positive rate:  0.4888888888888889\n",
            "Accuracy:  79.60893854748603\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       284\n",
            "           1       0.51      0.31      0.39        74\n",
            "\n",
            "    accuracy                           0.80       358\n",
            "   macro avg       0.67      0.62      0.63       358\n",
            "weighted avg       0.77      0.80      0.78       358\n",
            "\n",
            "Training Data : \n",
            "Recall/ Sensitivity:  0.7642163661581137\n",
            "Specificity:  0.8\n",
            "Precision:  0.9981884057971014\n",
            "False positive rate:  0.2\n",
            "Accuracy:  76.44628099173553\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      1.00      0.87       552\n",
            "           1       0.80      0.02      0.04       174\n",
            "\n",
            "    accuracy                           0.76       726\n",
            "   macro avg       0.78      0.51      0.46       726\n",
            "weighted avg       0.77      0.76      0.67       726\n",
            "\n",
            "Testing Data : \n",
            "Recall/ Sensitivity:  0.797752808988764\n",
            "Specificity:  1.0\n",
            "Precision:  1.0\n",
            "False positive rate:  0.0\n",
            "Accuracy:  79.88826815642457\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      1.00      0.89       284\n",
            "           1       1.00      0.03      0.05        74\n",
            "\n",
            "    accuracy                           0.80       358\n",
            "   macro avg       0.90      0.51      0.47       358\n",
            "weighted avg       0.84      0.80      0.71       358\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-37c5e1d8e47e>:23: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  print(\"Specificity: \",TN/(TN+FP))\n",
            "<ipython-input-32-37c5e1d8e47e>:27: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  print(\"False positive rate: \",FP/(FP+TN))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "<ipython-input-32-37c5e1d8e47e>:23: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  print(\"Specificity: \",TN/(TN+FP))\n",
            "<ipython-input-32-37c5e1d8e47e>:27: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  print(\"False positive rate: \",FP/(FP+TN))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'#Graphical User Interface'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rott6JR29FZl",
        "outputId": "4d06820b-2597-4f3a-9074-fbbf267e0b30"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.4.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.4-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.4.2 (from gradio)\n",
            "  Downloading gradio_client-1.4.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting huggingface-hub>=0.25.1 (from gradio)\n",
            "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart==0.0.12 (from gradio)\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.7.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<1.0,>=0.1.1 (from gradio)\n",
            "  Downloading safehttpx-0.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio) (2024.10.0)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.2->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.4.0-py3-none-any.whl (56.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.4.2-py3-none-any.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.4-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading orjson-3.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruff-0.7.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.1-py3-none-any.whl (8.4 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.2-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, orjson, markupsafe, h11, ffmpy, aiofiles, uvicorn, starlette, huggingface-hub, httpcore, httpx, fastapi, safehttpx, gradio-client, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.24.7\n",
            "    Uninstalling huggingface-hub-0.24.7:\n",
            "      Successfully uninstalled huggingface-hub-0.24.7\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.4 ffmpy-0.4.0 gradio-5.4.0 gradio-client-1.4.2 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 huggingface-hub-0.26.2 markupsafe-2.1.5 orjson-3.10.11 pydub-0.25.1 python-multipart-0.0.12 ruff-0.7.2 safehttpx-0.1.1 semantic-version-2.10.0 starlette-0.41.2 tomlkit-0.12.0 uvicorn-0.32.0 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import cv2\n",
        "import gradio as gr\n",
        "x_train_reshaped = x_train.reshape(-1, 224 * 224 * 1)\n",
        "\n",
        "# Define and fit PCA model\n",
        "\n",
        "def densenet_svm_gui(image):\n",
        "    image=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "    image = cv2.resize(image, default_image_size)\n",
        "    l=[]\n",
        "    l.append(img_to_array(image))\n",
        "    l = np.array(l, dtype=np.float16) / 225.0\n",
        "    data=np.reshape(l,(-1,224*224*1))\n",
        "    result=densenet_svm.predict(data)\n",
        "    if result==0:\n",
        "        result=\"Healthy\"\n",
        "    else:\n",
        "        result=\"Unhealthy\"\n",
        "    return result\n",
        "\n",
        "#built interface with gradio to test the function\n",
        "gr.Interface(fn=densenet_svm_gui, inputs=\"image\", outputs=\"text\",title='Densenet Model',allow_flagging=\"never\").launch(debug=True);\"\"\"\n",
        "import cv2\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Assuming x_train is already defined and preprocessed\n",
        "x_train_reshaped = x_train.reshape(-1, 224 * 224 * 1)\n",
        "\n",
        "# Define and fit PCA model\n",
        "pca_model = PCA(n_components=72)\n",
        "pca_model.fit(x_train_reshaped)\n",
        "\n",
        "def densenet_svm_gui(image):\n",
        "    # Convert image to grayscale\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    # Resize image to default size\n",
        "    image = cv2.resize(image, (224, 224))\n",
        "    # Convert to array and normalize\n",
        "    l = []\n",
        "    l.append(img_to_array(image))\n",
        "    l = np.array(l, dtype=np.float16) / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "    # Reshape the image to match the PCA model expectations\n",
        "    data = np.reshape(l, (-1, 224 * 224 * 1))\n",
        "\n",
        "    # Apply PCA to reduce to the expected number of features (72)\n",
        "    data = pca_model.transform(data)\n",
        "\n",
        "    # Check the shape of the data after PCA transformation\n",
        "    print(\"Shape of data after PCA:\", data.shape)\n",
        "\n",
        "    # Make prediction using the SVM model\n",
        "    result = densenet_svm.predict(data)\n",
        "    if result == 1:\n",
        "        result = \"Healthy\"\n",
        "    else:\n",
        "        result = \"Unhealthy\"\n",
        "    return result\n",
        "\n",
        "# Build interface with Gradio to test the function\n",
        "gr.Interface(fn=densenet_svm_gui, inputs=\"image\", outputs=\"text\", title='Coffee Leaf Disease Detection', allow_flagging=\"never\").launch(debug=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cQ75tlQzD_2I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "outputId": "863f5c7e-5406-438b-c0f6-8711b94dae2e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/interface.py:393: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://5716e45e6a51e650ce.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5716e45e6a51e650ce.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of data after PCA: (1, 72)\n",
            "Shape of data after PCA: (1, 72)\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://5716e45e6a51e650ce.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SHpwvzv8-jOM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}